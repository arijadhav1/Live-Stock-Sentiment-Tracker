{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new article found. Waiting for 5 minutes before checking again.\n",
      "Stop signal file detected. Exiting loop.\n",
      "                                            Headline  Positive  Negative  \\\n",
      "0  The benchmark index has hit new record highs t...  0.952614  0.022618   \n",
      "\n",
      "    Neutral  \n",
      "0  0.024768  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for web scraping, text processing, and data handling\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set up Chrome options for headless mode (optional, for running without a GUI)\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment to enable headless mode\n",
    "\n",
    "# Attempt to load the text of the last processed link to avoid repetition\n",
    "try:\n",
    "    with open('previous_link_text.txt', 'r') as file:\n",
    "        previous_link_text = file.read().strip()\n",
    "except FileNotFoundError:\n",
    "    previous_link_text = None\n",
    "\n",
    "# Define file path for saving the DataFrame as CSV\n",
    "df_file_path = 'scraped_articles.csv'\n",
    "\n",
    "# Check if the CSV file exists; if so, load it. Otherwise, initialize an empty DataFrame\n",
    "if os.path.exists(df_file_path):\n",
    "    df2 = pd.read_csv(df_file_path)\n",
    "else:\n",
    "    df2 = pd.DataFrame(columns=['Headline', 'Positive', 'Negative', 'Neutral'])\n",
    "\n",
    "# Flag to track if the process was interrupted\n",
    "interrupted = False\n",
    "\n",
    "# Prompt user for stock ticker input\n",
    "stock = input('What stock do you want to check the news for? ').strip()\n",
    "\n",
    "# Main try block to handle the workflow\n",
    "try:\n",
    "    while True:\n",
    "        # Use Chrome WebDriver with specified options\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            # Set timeouts for the driver\n",
    "            driver.implicitly_wait(10)\n",
    "            driver.set_script_timeout(120)\n",
    "            driver.set_page_load_timeout(90)\n",
    "\n",
    "            try:\n",
    "                # Navigate to Yahoo Finance news section for the specified stock\n",
    "                driver.get(f\"https://finance.yahoo.com/quote/{stock}/news?p={stock}\")\n",
    "                wait = WebDriverWait(driver, 10)\n",
    "\n",
    "                # Try to find the news link using CSS selectors\n",
    "                try:\n",
    "                    link = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.clamp.svelte-13zydns')))\n",
    "                except TimeoutException:\n",
    "                    try:\n",
    "                        link = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.StretchedBox')))\n",
    "                    except TimeoutException:\n",
    "                        print(\"Link not found with either class. Exiting.\")\n",
    "                        continue\n",
    "\n",
    "                # Check if the current link has already been processed\n",
    "                current_link_text = link.text\n",
    "                if current_link_text == previous_link_text:\n",
    "                    print(\"No new article found. Waiting for 5 minutes before checking again.\")\n",
    "                    for _ in range(30):  # Wait in 10-second intervals for 5 minutes\n",
    "                        if os.path.exists(\"stop_signal.txt\"):\n",
    "                            print(\"Stop signal file detected. Exiting loop.\")\n",
    "                            os.remove(\"stop_signal.txt\")\n",
    "                            interrupted = True\n",
    "                            break\n",
    "                        time.sleep(10)\n",
    "                    if interrupted:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                # Update and save the last processed link text\n",
    "                previous_link_text = current_link_text\n",
    "                with open('previous_link_text.txt', 'w') as file:\n",
    "                    file.write(previous_link_text)\n",
    "\n",
    "                # Click the link to navigate to the article\n",
    "                print(previous_link_text)\n",
    "                link.click()\n",
    "\n",
    "                # Try to expand the article content if \"Read More\" buttons are present\n",
    "                try:\n",
    "                    read_more_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"readmoreButtonText\")))\n",
    "                    read_more_button.click()\n",
    "                except TimeoutException:\n",
    "                    try:\n",
    "                        read_more_button = wait.until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, \".link.rapid-noclick-resp.caas-button.collapse-button\"))\n",
    "                        )\n",
    "                        read_more_button.click()\n",
    "                    except TimeoutException:\n",
    "                        print(\"No 'Read More' button found with either class name.\")\n",
    "\n",
    "                # Collect paragraphs from the article\n",
    "                paragraphs = wait.until(EC.presence_of_all_elements_located((By.TAG_NAME, 'p')))\n",
    "                final_par = ' '.join([par.text for par in paragraphs if par.text != '' and len(par.text.split()) > 15])\n",
    "\n",
    "                # Initialize tokenizer and model for sentiment analysis\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "                # Tokenize the article and predict sentiment\n",
    "                inputs = tokenizer(final_par, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "                # Extract sentiment scores\n",
    "                positive, negative, neutral = predictions[:, 0].tolist(), predictions[:, 1].tolist(), predictions[:, 2].tolist()\n",
    "\n",
    "                # Update the DataFrame with new article information\n",
    "                table = {'Headline': final_par, \"Positive\": positive, \"Negative\": negative, \"Neutral\": neutral}\n",
    "                df = pd.DataFrame(table, index=[0])\n",
    "                df2 = pd.concat([df2, df], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            # Check for a stop signal to end the loop\n",
    "            if os.path.exists(\"stop_signal.txt\"):\n",
    "                print(\"Stop signal file detected. Exiting loop.\")\n",
    "                os.remove(\"stop_signal.txt\")\n",
    "                break\n",
    "\n",
    "            # Wait before checking for new articles again\n",
    "            print(\"Waiting a short while before checking for new articles again...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "finally:\n",
    "    # Save the updated link text and DataFrame before exiting\n",
    "    with open('previous_link_text.txt', 'w') as file:\n",
    "        file.write(previous_link_text if previous_link_text else \"\")\n",
    "    df2.to_csv('scraped_articles.csv', index=False)\n",
    "    print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
